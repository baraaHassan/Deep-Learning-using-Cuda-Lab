{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cuda Vision Lab Assignment 1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-WKkmSJyBSR"
      },
      "source": [
        "### Assignment 1:\n",
        "* Build 2 layer network for Logistic Regression Classifier using pure python/numpy\n",
        "  * Input data x : 20 x 784 (20 is the batch size)\n",
        "  * label y : 20 x 10\n",
        "  * Weight Variable W with random_normal initialization\n",
        "  * Bias variable b with zeros\n",
        "  * Use softmax activation => softmax(x.W+b)\n",
        "  * Use OneHot encoding and MSE loss! Not the best way for classification but just for practice\n",
        "  * Train the model on MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMmjjbvlyoAW"
      },
      "source": [
        "import os, struct\n",
        "from array import array as pyarray\n",
        "from numpy import  array, zeros\n",
        "import numpy as np\n",
        "import random  \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2EelaRJB9d7"
      },
      "source": [
        "The first function is to load the MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihytCzrmCDfx"
      },
      "source": [
        "def load_mnist(dataset=\"training\", digits=np.arange(10), path=\".\"):\n",
        "    \"\"\"\n",
        "    Loads MNIST files into numpy arrays \n",
        "    \"\"\"\n",
        "\n",
        "    if dataset == \"training\":\n",
        "        fname_img = os.path.join('sample_data', 'train-images.idx3-ubyte')\n",
        "        fname_lbl = os.path.join('sample_data', 'train-labels.idx1-ubyte')\n",
        "    elif dataset == \"testing\":\n",
        "        fname_img = os.path.join('sample_data', 't10k-images.idx3-ubyte')\n",
        "        fname_lbl = os.path.join('sample_data', 't10k-labels.idx1-ubyte')\n",
        "    else:\n",
        "        raise ValueError(\"dataset must be 'testing' or 'training'\")\n",
        "\n",
        "    flbl = open(fname_lbl, 'rb')\n",
        "    magic_nr, size = struct.unpack(\">II\", flbl.read(8))\n",
        "    lbl = pyarray(\"b\", flbl.read())\n",
        "    flbl.close()\n",
        "\n",
        "    fimg = open(fname_img, 'rb')\n",
        "    magic_nr, size, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n",
        "    img = pyarray(\"B\", fimg.read())\n",
        "    fimg.close()\n",
        "\n",
        "    ind = [ k for k in range(size) if lbl[k] in digits ]\n",
        "    len_of_index = len(ind)\n",
        "\n",
        "    random.seed(30)\n",
        "    #random number in the range of the dataset - to shufftle the dataset\n",
        "    rand_num = random.randrange(len_of_index - 20)\n",
        "    # 20 is the batch size\n",
        "    images = zeros((20, rows, cols) )\n",
        "    labels = zeros((20))\n",
        "    for i in range(20):\n",
        "        images[i] = array(img[ ind[rand_num+i]*rows*cols : (ind[rand_num+i]+1)*rows*cols ]).reshape((rows, cols))\n",
        "        labels[i] = lbl[ind[rand_num+i]]\n",
        "\n",
        "    return images, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO05NbObEYgB"
      },
      "source": [
        "2nd fuction to calculate the softmax function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOUfr6wHEf-3"
      },
      "source": [
        "def softmax(z_matrix):\n",
        "    number_of_samples,number_of_classes = z_matrix.shape\n",
        "    expected_output = np.zeros(z_matrix.shape)\n",
        "    \n",
        "    for i in range(number_of_samples):    \n",
        "        normalization_value = 0\n",
        "        for j in range(number_of_classes):\n",
        "            expected_output[i][j] = np.exp(z_matrix[i][j])\n",
        "            normalization_value += expected_output[i][j] \n",
        "        expected_output[i] /= normalization_value\n",
        "    return expected_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxwW5T58Eiu0"
      },
      "source": [
        "3rd function to calculate the gradient descent for each iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nWfC4gnEs0Q"
      },
      "source": [
        "def Gradient_Descent_for_one_sample(actual_y,y_prime,input_x):\n",
        "    number_of_classes = len(actual_y)\n",
        "    dl_dy =  actual_y - y_prime\n",
        "    dy_dz = np.zeros((number_of_classes,number_of_classes))\n",
        "    \n",
        "    for i in range(number_of_classes): \n",
        "        for j in range(number_of_classes):\n",
        "            if(i == j):\n",
        "                sij_value = 1\n",
        "            else:\n",
        "                sij_value = 0\n",
        "            dy_dz[i,j] = y_prime[i] * (sij_value - y_prime[j])\n",
        "            \n",
        "    dz_dw = input_x.reshape((input_x.shape[0],1))\n",
        "    gradient_b = dl_dy.reshape((1,number_of_classes))@dy_dz\n",
        "    gradient_w = dz_dw@gradient_b\n",
        "    return gradient_b,gradient_w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvKBMAyTCamQ"
      },
      "source": [
        "Then we will load the the Mnist dataset into arrays:\n",
        " x_train (for the features) and temp_y_train (as a temp array for the labels)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHfv4XJsCwC1"
      },
      "source": [
        "# Read in training data as features and labels\n",
        "X_train, temp_y_train = load_mnist('training') \n",
        "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
        "X_train = np.divide(X_train, 256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1fALgPHDB2a"
      },
      "source": [
        "Customizing the y_train array to fit into our classification problem, as the output should be list of probabilities of 10 classes\n",
        "(so the position of the class where this label related to  = 1 and the positions related to other classes = 0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfxbxW-aDTdX"
      },
      "source": [
        "number_of_samples = 20\n",
        "number_of_classes = 10\n",
        "\n",
        "# change triainging and testing to fit softmax regression\n",
        "y_train = np.zeros((number_of_samples,number_of_classes))\n",
        "\n",
        "for i in range(20):\n",
        "    y_train[i][int(temp_y_train[i])] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGyCgG9OE8vS"
      },
      "source": [
        "Then the training step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtVPn15pFebT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "df597275-7902-4e22-e533-be75f0db39a9"
      },
      "source": [
        "# Intializing the weight with random values drawn from a normal distribution with \"mean = 0\" & \"sigma = 0.1\"  \n",
        "mu, sigma = 0, 0.1\n",
        "np.random.seed(1)\n",
        "W = np.random.normal(mu, sigma, (X_train.shape[1],10))\n",
        "b = np.zeros((1,10))\n",
        "number_of_epochs = 1000\n",
        "\n",
        "for k in range(number_of_epochs):\n",
        "    # feed forward\n",
        "    #layer 0\n",
        "    l0 = X_train\n",
        "    # layer 1\n",
        "    l1 = l0@W+b\n",
        "    # Expected output\n",
        "    expected_output = softmax(l1)\n",
        "    \n",
        "    delta_w = np.zeros(W.shape)\n",
        "    delta_b = np.zeros(b.shape)\n",
        "    \n",
        "    # backpropagate - update weight\n",
        "    for i in range (number_of_samples):\n",
        "        d_b,d_w = Gradient_Descent_for_one_sample(y_train[i],expected_output[i],X_train[i])\n",
        "        delta_w += d_w\n",
        "        delta_b += d_b\n",
        "        \n",
        "    delta_w = (-2/number_of_samples) * delta_w\n",
        "    delta_b = (-2/number_of_samples) * delta_b\n",
        "    \n",
        "    W -= delta_w\n",
        "    b -= delta_b\n",
        "  \n",
        "for i in range (number_of_samples):\n",
        "  print (\"expect output\",expected_output[i])\n",
        "  print (\"labels\",y_train[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "expect output [1.07937170e-04 9.88611770e-01 1.04859554e-03 6.29502249e-04\n",
            " 5.04003334e-04 2.32558328e-04 2.89909455e-04 9.54906880e-04\n",
            " 1.68314737e-03 5.93766978e-03]\n",
            "labels [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "expect output [3.02587669e-04 3.66486443e-04 9.71760502e-04 3.33207083e-03\n",
            " 9.80502057e-04 6.38416201e-04 7.98230839e-04 9.30314178e-04\n",
            " 1.87370765e-03 9.89805924e-01]\n",
            "labels [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "expect output [1.84458535e-03 5.85274253e-04 9.90849851e-01 7.38923553e-04\n",
            " 7.74642448e-04 4.96561747e-05 7.02489918e-04 1.42000666e-03\n",
            " 3.00988698e-03 2.46832068e-05]\n",
            "labels [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "expect output [2.67795759e-04 4.43470422e-03 1.04355251e-03 1.59288916e-03\n",
            " 8.59068100e-04 5.63235755e-04 6.32645087e-04 3.23858352e-03\n",
            " 1.98400457e-03 9.85383521e-01]\n",
            "labels [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "expect output [3.79214133e-04 3.52447809e-05 6.63762289e-04 9.92459219e-01\n",
            " 1.26715791e-05 2.94856387e-05 1.21394306e-04 2.71503367e-04\n",
            " 2.37384126e-03 3.65366343e-03]\n",
            "labels [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "expect output [1.67297928e-04 2.40264232e-04 2.13114557e-03 3.10407546e-04\n",
            " 3.26261900e-04 2.52909750e-04 3.19079606e-04 9.90555479e-01\n",
            " 5.47197515e-04 5.14995726e-03]\n",
            "labels [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "expect output [5.50870998e-04 2.47396016e-04 7.72973360e-04 2.03376024e-04\n",
            " 9.91134423e-01 1.16735708e-04 1.04116355e-03 6.01443076e-04\n",
            " 1.50057747e-03 3.83104052e-03]\n",
            "labels [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "expect output [6.59751241e-04 9.28401374e-04 1.34285446e-03 9.90730420e-01\n",
            " 3.96255194e-04 2.36176894e-04 8.96879592e-04 2.30747070e-04\n",
            " 2.44534139e-03 2.13317292e-03]\n",
            "labels [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "expect output [9.24800460e-05 1.61057948e-03 3.66800843e-03 1.27101319e-04\n",
            " 3.45920862e-04 1.00627563e-04 1.47564831e-04 9.91788059e-01\n",
            " 1.90083337e-04 1.92957523e-03]\n",
            "labels [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "expect output [5.30760596e-04 2.71076844e-03 2.03641522e-03 3.31631544e-03\n",
            " 1.26235221e-04 5.51137067e-05 7.55671663e-04 3.71120996e-04\n",
            " 9.89262123e-01 8.35475734e-04]\n",
            "labels [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "expect output [6.47827475e-04 3.17943108e-04 4.04997257e-03 7.52535327e-04\n",
            " 8.45249092e-04 2.31681511e-05 5.23383093e-04 9.99747081e-04\n",
            " 9.91424413e-01 4.15761574e-04]\n",
            "labels [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "expect output [1.13120198e-03 1.43963999e-03 9.89753897e-01 7.91499117e-04\n",
            " 1.93448170e-04 4.39853533e-04 1.56181307e-03 2.42333989e-03\n",
            " 1.55326157e-03 7.12045325e-04]\n",
            "labels [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "expect output [9.39232831e-05 1.50596423e-03 8.55448712e-04 3.80799347e-04\n",
            " 9.70061201e-04 1.93061503e-04 3.68773315e-04 2.44027310e-03\n",
            " 3.46381679e-04 9.92845314e-01]\n",
            "labels [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "expect output [1.06465726e-03 4.58860811e-04 3.61778438e-03 5.29781988e-04\n",
            " 8.82102468e-04 1.50600730e-04 9.89791911e-01 3.88181221e-04\n",
            " 2.42008797e-03 6.96031879e-04]\n",
            "labels [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "expect output [9.93024649e-01 6.52927223e-05 2.03454884e-03 2.33792769e-03\n",
            " 5.69573996e-04 1.08988695e-04 1.23041685e-03 3.88435659e-05\n",
            " 5.41099185e-04 4.86596135e-05]\n",
            "labels [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "expect output [6.26564589e-04 6.80195757e-04 9.91846270e-01 2.25425095e-03\n",
            " 2.93349184e-04 3.60087569e-04 5.79474664e-04 1.32123874e-03\n",
            " 1.82419581e-03 2.14372635e-04]\n",
            "labels [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "expect output [1.82769358e-04 9.93420254e-01 2.46033174e-03 6.25745935e-04\n",
            " 2.40667732e-04 2.08046487e-04 1.31659253e-04 3.03194901e-04\n",
            " 1.48287268e-03 9.44458001e-04]\n",
            "labels [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "expect output [6.05142759e-05 4.71255348e-04 9.74692835e-04 9.96137848e-01\n",
            " 1.03795556e-04 4.40256870e-05 5.48577937e-05 2.39172614e-05\n",
            " 3.72703288e-04 1.75638998e-03]\n",
            "labels [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "expect output [7.15983554e-05 6.70741716e-05 9.99180442e-01 5.40185830e-05\n",
            " 6.54927279e-05 1.53472248e-05 9.07384214e-05 2.13919274e-04\n",
            " 2.07851229e-04 3.35178832e-05]\n",
            "labels [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "expect output [9.94761776e-01 2.04256654e-05 2.69855676e-03 7.21423354e-04\n",
            " 3.58792391e-04 3.22243545e-05 7.03659233e-04 6.39210093e-05\n",
            " 6.37832285e-04 1.38863519e-06]\n",
            "labels [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}